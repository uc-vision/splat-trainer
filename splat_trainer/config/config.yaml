# @package _global_

defaults:
  - _self_
  - dataset: scan

trainer:
  _target_: splat_trainer.TrainConfig
  num_iterations: 300000
  val_interval: 1000
  feature_learning_rate: 1e-3
  iteration_start_camera_pose_optimization: 30000
  iteration_start_depth_cov_loss: 2000
  enable_depth_cov_loss: True
  initial_depth_cov_loss_factor:  0.4
  depth_cov_loss_factor_increase_interval: 1000
  depth_cov_loss_factor_increase_rate: 1.05511
  camera_pose_optimization_batch_size: 500
  position_learning_rate: 1e-5
  position_learning_rate_decay_rate: 0.97
  position_learning_rate_decay_interval: 100
  camera_pose_learning_rate_decay_rate:  0.97
  increase_color_max_sh_band_interval: 1000.
  camera_pose_learning_rate: 1e-6
  log_loss_interval: 10
  log_metrics_interval: 100
  print_metrics_to_console: False
  log_image_interval: 1000
  enable_taichi_kernel_profiler: False
  log_taichi_kernel_profile_interval: 1000
  log_validation_image: True
  initial_downsample_factor: 4
  half_downsample_factor_interval: 250
  
  output_model_dir: "guassian"

  adaptive_controller_config: ${..controller_config}
  rasterisation_config: ${..rasterisation_config}


rasterisation_config:
    _target_: splat_trainer.RasterisationConfig
    near_plane: 0.1
    far_plane: 100.
    depth_to_sort_key_scale: 10000.
    rgb_only: False
    enable_grad_camera_pose: False
    # grad_color_factor: 5.
    # grad_high_order_color_factor: 1.
    # grad_s_factor: 0.5
    # grad_q_factor: 1.
    # grad_alpha_factor: 20.


controller_config:
  _target_: splat_trainer.ControllerConfig
  num_iterations_warm_up:  500
  num_iterations_densify:  100
  # from paper: densify every 100 iterations and remove any Gaussians that are essentially transparent, i.e., with ùõº less than a threshold ùúñùõº.
  transparent_alpha_threshold: -0.5
  # from paper: densify Gaussians with an average magnitude of view-space position gradients above a threshold ùúèpos, which we set to 0.0002 in our tests.
  # I have no idea why their threshold is so low, may be their view space is normalized to [0, 1]?
  # TODO: find out a proper threshold
  densification_view_space_position_gradients_threshold:  6e-6
  densification_view_avg_space_position_gradients_threshold:  1e3
  densification_multi_frame_view_space_position_gradients_threshold: 1e3
  densification_multi_frame_view_pixel_avg_space_position_gradients_threshold: 1e3
  densification_multi_frame_position_gradients_threshold: 1e3
  # from paper:  large Gaussians in regions with high variance need to be split into smaller Gaussians. We replace such Gaussians by two new ones, and divide their scale by a factor of ùúô = 1.6
  gaussian_split_factor_phi: 1.6
  # in paper section 5.2, they describe a method to moderate the increase in the number of Gaussians is to set the ùõº value close to zero every
  # 3000 iterations. I have no idea how it is implemented. I just assume that it is a reset of ùõº to fixed value.
  num_iterations_reset_alpha:  3000
  reset_alpha_value: 0.1
  # the paper doesn't mention this value, but we need a value and method to determine whether a point is under-reconstructed or over-reconstructed
  # for now, the method is to threshold norm of exp(s)
  # TODO: find out a proper threshold
  floater_num_pixels_threshold: 10000
  floater_near_camrea_num_pixels_threshold: 10000
  floater_depth_threshold: 100
  iteration_start_remove_floater: 2000
  plot_densify_interval: 200
  under_reconstructed_num_pixels_threshold: 512
  under_reconstructed_move_factor: 100.0
  enable_ellipsoid_offset: False
  enable_sample_from_point: True

loss: 
  _target_: splat_trainer.LossFunction
  lambda_value:  0.2
  enable_regularization: True
  regularization_weight:  2

scene_config:
  _target_: splat_trainer.SceneConfig
  num_of_features:  56
  # max_num_points_ratio: None
  add_sphere: False
  sphere_radius_factor: 4.0
  num_points_sphere: 10000
  # max_initial_covariance: None
  initial_alpha: ${inverse_sigmoid:0.5}
  initial_covariance_ratio: 1.0